\chapter{Machine Learning for Financial Time Series}
\label{ch:machine_learning}

\section{Introduction to Machine Learning in Finance}
\label{sec:ml_intro}

Machine learning has revolutionized quantitative finance, providing powerful tools for prediction, pattern recognition, and automated decision-making. This chapter focuses on time series analysis and volatility modeling, with particular emphasis on GARCH models and deep learning approaches.

\subsection{Machine Learning Paradigms}
\label{subsec:ml_paradigms}

\textbf{Supervised Learning:} Learning from labeled data $(x_i, y_i)$
\begin{itemize}
    \item \textbf{Regression:} Predict continuous values (returns, prices, volatility)
    \item \textbf{Classification:} Predict categorical outcomes (up/down, buy/sell/hold)
\end{itemize}

\textbf{Unsupervised Learning:} Finding structure in unlabeled data
\begin{itemize}
    \item \textbf{Clustering:} Grouping similar assets or market regimes
    \item \textbf{Dimensionality reduction:} PCA for factor models
\end{itemize}

\textbf{Reinforcement Learning:} Learning optimal trading policies through interaction

\subsection{Financial Time Series Characteristics}
\label{subsec:ts_characteristics}

Financial time series exhibit unique properties:

\begin{enumerate}
    \item \textbf{Non-stationarity:} Statistical properties change over time
    \item \textbf{Volatility clustering:} Periods of high/low volatility persist
    \item \textbf{Heavy tails:} More extreme events than normal distribution
    \item \textbf{Autocorrelation:} Returns have little autocorrelation, but $|r_t|$ and $r_t^2$ do
    \item \textbf{Leverage effect:} Volatility increases more after negative returns
\end{enumerate}

\section{Time Series Modeling}
\label{sec:time_series}

\subsection{Autoregressive Models}
\label{subsec:ar_models}

\begin{definition}[AR(p) Process]
An autoregressive process of order $p$ is:
\[
X_t = c + \sum_{i=1}^p \phi_i X_{t-i} + \epsilon_t, \quad \epsilon_t \sim WN(0, \sigma^2)
\]
where $WN$ denotes white noise.
\end{definition}

\textbf{Stationarity Condition:} All roots of $1 - \phi_1 z - \cdots - \phi_p z^p = 0$ must lie outside the unit circle.

\subsubsection{AR(1) Properties}

For $X_t = \phi X_{t-1} + \epsilon_t$ with $|\phi| < 1$:
\begin{align}
\E[X_t] &= 0 \\
\Var(X_t) &= \frac{\sigma^2}{1 - \phi^2} \\
\text{Corr}(X_t, X_{t-k}) &= \phi^k
\end{align}

\textbf{Half-life:} Time for autocorrelation to decay to 0.5:
\[
t_{1/2} = \frac{\ln(0.5)}{\ln(\phi)}
\]

\subsection{Moving Average Models}
\label{subsec:ma_models}

\begin{definition}[MA(q) Process]
A moving average process of order $q$ is:
\[
X_t = \mu + \epsilon_t + \sum_{i=1}^q \theta_i \epsilon_{t-i}, \quad \epsilon_t \sim WN(0, \sigma^2)
\]
\end{definition}

\textbf{Properties:} MA(q) processes are always stationary and have autocorrelation that cuts off after lag $q$.

\subsection{ARMA and ARIMA Models}
\label{subsec:arima}

\begin{definition}[ARMA(p,q) Process]
\[
X_t = c + \sum_{i=1}^p \phi_i X_{t-i} + \epsilon_t + \sum_{j=1}^q \theta_j \epsilon_{t-j}
\]
\end{definition}

\begin{definition}[ARIMA(p,d,q) Process]
An integrated ARMA process where the series is differenced $d$ times:
\[
(1-B)^d X_t = \text{ARMA}(p,q)
\]
where $B$ is the backshift operator: $BX_t = X_{t-1}$.
\end{definition}

\textbf{Model Selection:} Use ACF and PACF plots:
\begin{itemize}
    \item \textbf{AR(p):} ACF decays, PACF cuts off at lag $p$
    \item \textbf{MA(q):} ACF cuts off at lag $q$, PACF decays
    \item \textbf{ARMA(p,q):} Both decay
\end{itemize}

\textbf{Information Criteria:}
\begin{align}
\text{AIC} &= -2\log L + 2k \\
\text{BIC} &= -2\log L + k\log n
\end{align}
where $L$ is likelihood, $k$ is number of parameters, $n$ is sample size.

\section{Volatility Modeling with GARCH}
\label{sec:garch}

Volatility is not constant and exhibits clustering. GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models capture this.

\subsection{ARCH Model}
\label{subsec:arch}

\begin{definition}[ARCH(q)]
The ARCH(q) model by Engle (1982):
\[
\epsilon_t = \sigma_t z_t, \quad z_t \sim \text{i.i.d.} \, (0,1)
\]
\[
\sigma_t^2 = \omega + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2
\]
\end{definition}

\textbf{Interpretation:} Current volatility depends on past squared shocks.

\textbf{Constraints:} $\omega > 0$, $\alpha_i \geq 0$ to ensure $\sigma_t^2 > 0$.

\subsection{GARCH Model}
\label{subsec:garch_model}

\begin{definition}[GARCH(p,q)]
The GARCH model by Bollerslev (1986):
\[
\sigma_t^2 = \omega + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 + \sum_{j=1}^p \beta_j \sigma_{t-j}^2
\]
\end{definition}

The GARCH(1,1) is most commonly used:
\[
\sigma_t^2 = \omega + \alpha \epsilon_{t-1}^2 + \beta \sigma_{t-1}^2
\]

\subsubsection{GARCH(1,1) Properties}

\textbf{Unconditional Variance:} If $\alpha + \beta < 1$ (stationarity):
\[
\E[\sigma_t^2] = \frac{\omega}{1 - \alpha - \beta}
\]

\textbf{Persistence:} $\alpha + \beta$ measures volatility persistence
\begin{itemize}
    \item If $\alpha + \beta \approx 1$: high persistence (IGARCH)
    \item If $\alpha + \beta < 1$: mean-reverting volatility
\end{itemize}

\textbf{Half-life of Volatility Shocks:}
\[
t_{1/2} = \frac{\ln(0.5)}{\ln(\alpha + \beta)}
\]

\textbf{Kurtosis:} GARCH generates fat tails:
\[
\text{Kurt}(\epsilon_t) = 3 \frac{1 - (\alpha+\beta)^2}{1 - (\alpha+\beta)^2 - 2\alpha^2}
\]

\subsection{Estimation}
\label{subsec:garch_estimation}

GARCH parameters are estimated by maximum likelihood.

\textbf{Log-Likelihood:} Assuming normal innovations:
\[
\log L = -\frac{T}{2}\log(2\pi) - \frac{1}{2}\sum_{t=1}^T \left(\log\sigma_t^2 + \frac{\epsilon_t^2}{\sigma_t^2}\right)
\]

\textbf{Optimization:} Use numerical methods (BFGS, Newton-Raphson) to maximize $\log L$ subject to constraints.

\subsection{GJR-GARCH (Asymmetric GARCH)}
\label{subsec:gjr_garch}

The GJR-GARCH model by Glosten, Jagannathan, and Runkle (1993) captures the leverage effect.

\begin{definition}[GJR-GARCH(1,1)]
\[
\sigma_t^2 = \omega + (\alpha + \gamma I_{t-1})\epsilon_{t-1}^2 + \beta \sigma_{t-1}^2
\]
where $I_{t-1} = 1$ if $\epsilon_{t-1} < 0$, and $I_{t-1} = 0$ otherwise.
\end{definition}

\textbf{Interpretation:}
\begin{itemize}
    \item Positive shocks: impact is $\alpha$
    \item Negative shocks: impact is $\alpha + \gamma$
    \item If $\gamma > 0$: leverage effect (bad news increases volatility more)
\end{itemize}

\subsection{Forecasting with GARCH}
\label{subsec:garch_forecast}

\textbf{One-step ahead forecast:}
\[
\hat{\sigma}_{T+1}^2 = \omega + \alpha \epsilon_T^2 + \beta \sigma_T^2
\]

\textbf{Multi-step forecast:} For GARCH(1,1):
\[
\hat{\sigma}_{T+h}^2 = \frac{\omega}{1-\alpha-\beta} + (\alpha+\beta)^{h-1}\left(\hat{\sigma}_{T+1}^2 - \frac{\omega}{1-\alpha-\beta}\right)
\]

As $h \to \infty$, forecast converges to unconditional variance.

\subsection{Applications in Finance}
\label{subsec:garch_applications}

\begin{enumerate}
    \item \textbf{Risk Management:} VaR and ES calculation with time-varying volatility
    \item \textbf{Option Pricing:} Dynamic volatility improves pricing accuracy
    \item \textbf{Portfolio Optimization:} Conditional covariance matrices
    \item \textbf{Volatility Trading:} VIX futures, variance swaps
\end{enumerate}

\section{Deep Learning for Finance}
\label{sec:deep_learning}

\subsection{Neural Networks Fundamentals}
\label{subsec:nn_fundamentals}

\begin{definition}[Feedforward Neural Network]
A network with $L$ layers:
\begin{align}
h^{(0)} &= x \quad \text{(input)} \\
h^{(l)} &= g(W^{(l)} h^{(l-1)} + b^{(l)}), \quad l = 1, \ldots, L-1 \\
\hat{y} &= W^{(L)} h^{(L-1)} + b^{(L)} \quad \text{(output)}
\end{align}
where $g(\cdot)$ is an activation function (ReLU, tanh, sigmoid).
\end{definition}

\textbf{Universal Approximation:} Neural networks with one hidden layer can approximate any continuous function on compact domains.

\subsection{Recurrent Neural Networks}
\label{subsec:rnn}

For sequential data, RNNs maintain hidden state:
\begin{align}
h_t &= g(W_h h_{t-1} + W_x x_t + b_h) \\
y_t &= W_y h_t + b_y
\end{align}

\textbf{Limitation:} Vanishing/exploding gradients in long sequences.

\subsection{Long Short-Term Memory Networks}
\label{subsec:lstm}

LSTM by Hochreiter and Schmidhuber (1997) solves the vanishing gradient problem.

\begin{definition}[LSTM Cell]
An LSTM cell has three gates:
\begin{align}
f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \quad \text{(forget gate)} \\
i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \quad \text{(input gate)} \\
o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \quad \text{(output gate)} \\
\tilde{C}_t &= \tanh(W_C [h_{t-1}, x_t] + b_C) \quad \text{(candidate cell state)} \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \quad \text{(cell state)} \\
h_t &= o_t \odot \tanh(C_t) \quad \text{(hidden state)}
\end{align}
where $\sigma(\cdot)$ is the sigmoid function and $\odot$ is element-wise multiplication.
\end{definition}

\textbf{Key Features:}
\begin{itemize}
    \item \textbf{Cell state:} Allows information to flow unchanged
    \item \textbf{Gates:} Control what information to keep/forget
    \item \textbf{Long-term dependencies:} Can learn patterns across many time steps
\end{itemize}

\subsection{LSTM for Financial Forecasting}
\label{subsec:lstm_finance}

\subsubsection{Architecture Design}

For return prediction with lookback window of $T$ days:
\begin{enumerate}
    \item \textbf{Input:} Sequence of past returns $(r_{t-T}, \ldots, r_{t-1})$
    \item \textbf{LSTM Layers:} 1-3 layers with 50-200 units
    \item \textbf{Dropout:} 0.2-0.5 to prevent overfitting
    \item \textbf{Output:} Dense layer for $\hat{r}_t$
\end{enumerate}

\subsubsection{Training Procedure}

\textbf{Loss Function:} Mean squared error
\[
\mathcal{L} = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2
\]

\textbf{Optimization:} Adam optimizer
\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1)g_t \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\
\theta_t &= \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}
where $g_t$ is gradient, typical values: $\beta_1=0.9$, $\beta_2=0.999$, $\alpha=0.001$.

\textbf{Regularization:}
\begin{itemize}
    \item Dropout: randomly zero activations
    \item Early stopping: monitor validation loss
    \item Batch normalization: normalize layer inputs
\end{itemize}

\subsubsection{Financial Time Series Considerations}

\begin{enumerate}
    \item \textbf{Train-Test Split:} Must be chronological (no random shuffle!)
    \[
    \text{Train: } t = 1, \ldots, T_{\text{train}}, \quad \text{Test: } t = T_{\text{train}}+1, \ldots, T
    \]
    
    \item \textbf{Walk-Forward Validation:}
    \begin{itemize}
        \item Train on $[1, t]$
        \item Test on $[t+1, t+h]$
        \item Slide window forward
    \end{itemize}
    
    \item \textbf{Feature Engineering:}
    \begin{itemize}
        \item Technical indicators (moving averages, RSI, MACD)
        \item Lagged returns
        \item Realized volatility
        \item Volume and order flow
    \end{itemize}
    
    \item \textbf{Data Scaling:} Normalize inputs
    \[
    x_{\text{scaled}} = \frac{x - \mu}{\sigma}
    \]
\end{enumerate}

\subsection{Performance Evaluation}
\label{subsec:ml_evaluation}

\textbf{Regression Metrics:}
\begin{align}
\text{MSE} &= \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
\text{MAE} &= \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i| \\
R^2 &= 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}
\end{align}

\textbf{Trading Performance:}
\begin{itemize}
    \item Sharpe ratio of strategy based on predictions
    \item Hit rate: proportion of correct directional predictions
    \item Information coefficient: correlation between predictions and actual returns
\end{itemize}

\textbf{Statistical Tests:}
\begin{itemize}
    \item Diebold-Mariano test for forecast accuracy comparison
    \item Out-of-sample $R^2$ (Campbell-Thompson test)
\end{itemize}

\section{Combining GARCH and LSTM}
\label{sec:hybrid_models}

\subsection{Hybrid Architecture}
\label{subsec:hybrid_arch}

Combine strengths of both approaches:

\textbf{Stage 1—GARCH:} Model volatility
\[
\sigma_t^2 = \omega + \alpha \epsilon_{t-1}^2 + \beta \sigma_{t-1}^2
\]

\textbf{Stage 2—LSTM:} Use GARCH volatility as feature
\[
\hat{r}_t = f_{\text{LSTM}}(r_{t-1}, \ldots, r_{t-T}, \sigma_t)
\]

\textbf{Advantages:}
\begin{itemize}
    \item GARCH provides interpretable volatility estimates
    \item LSTM captures nonlinear patterns
    \item Volatility regime information improves predictions
\end{itemize}

\subsection{Model Ensembles}
\label{subsec:ensembles}

Combine multiple models to reduce variance:
\begin{itemize}
    \item \textbf{Simple average:} $\hat{y} = \frac{1}{M}\sum_{m=1}^M \hat{y}_m$
    \item \textbf{Weighted average:} $\hat{y} = \sum_{m=1}^M w_m \hat{y}_m$ where $w_m$ based on validation performance
    \item \textbf{Stacking:} Train meta-model on base model predictions
\end{itemize}

\section{Practical Considerations}
\label{sec:ml_practical}

\subsection{Overfitting and Regularization}
\label{subsec:overfitting}

\textbf{Signs of Overfitting:}
\begin{itemize}
    \item Large gap between train and validation error
    \item High in-sample $R^2$, poor out-of-sample performance
    \item Model too complex relative to data
\end{itemize}

\textbf{Mitigation:}
\begin{enumerate}
    \item Cross-validation (time series-aware)
    \item L1/L2 regularization
    \item Early stopping
    \item Dropout
    \item Reduce model complexity
\end{enumerate}

\subsection{Data Quality and Preprocessing}
\label{subsec:data_quality}

\begin{itemize}
    \item \textbf{Missing data:} Forward fill for prices, zero for returns
    \item \textbf{Outliers:} Winsorize extreme values
    \item \textbf{Survivorship bias:} Include delisted stocks
    \item \textbf{Look-ahead bias:} Ensure data availability at time of prediction
\end{itemize}

\subsection{Computational Efficiency}
\label{subsec:efficiency}

\textbf{GPU Acceleration:} Use TensorFlow/PyTorch with CUDA
\begin{itemize}
    \item 10-100x speedup for large models
    \item Batch processing
\end{itemize}

\textbf{Model Compression:}
\begin{itemize}
    \item Pruning: remove low-importance weights
    \item Quantization: reduce precision (float32 $\to$ int8)
    \item Knowledge distillation: train smaller model on large model outputs
\end{itemize}

\section{Limitations and Critiques}
\label{sec:ml_limitations}

\subsection{Theoretical Limitations}
\label{subsec:theoretical_limits}

\begin{enumerate}
    \item \textbf{Efficient Market Hypothesis:} If markets are efficient, predictability is limited
    \item \textbf{Non-stationarity:} Relationships change over time
    \item \textbf{Low signal-to-noise ratio:} Financial data is inherently noisy
    \item \textbf{Black box:} Neural networks lack interpretability
\end{enumerate}

\subsection{Practical Challenges}
\label{subsec:practical_challenges}

\begin{itemize}
    \item \textbf{Transaction costs:} High-frequency predictions may not be profitable after costs
    \item \textbf{Market impact:} Large trades move prices
    \item \textbf{Regime changes:} Models trained on one regime fail in another
    \item \textbf{Data snooping:} Multiple testing inflates apparent performance
\end{itemize}

\subsection{Best Practices}
\label{subsec:best_practices}

\begin{enumerate}
    \item Start simple (linear models, GARCH) before deep learning
    \item Use walk-forward validation
    \item Account for transaction costs
    \item Ensemble multiple models
    \item Monitor model performance in production
    \item Retrain periodically with new data
    \item Maintain realistic expectations
\end{enumerate}

This chapter has provided a comprehensive treatment of machine learning methods for financial time series, with emphasis on volatility modeling and deep learning. The combination of traditional econometric models (GARCH) and modern machine learning (LSTM) represents the state-of-the-art in quantitative finance. The final chapter will apply these tools to portfolio optimization and risk management.
