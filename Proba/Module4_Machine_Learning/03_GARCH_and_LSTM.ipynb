{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc765651",
   "metadata": {},
   "source": [
    "# Module 4: GARCH Models and LSTM for Financial Time Series\n",
    "\n",
    "**CRITICAL FOR QUANTITATIVE FINANCE**\n",
    "\n",
    "This notebook covers volatility modeling with GARCH and deep learning with LSTM for financial forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330562da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from arch import arch_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"GARCH and LSTM modules loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f460555",
   "metadata": {},
   "source": [
    "## 1. GARCH Models for Volatility\n",
    "\n",
    "**GARCH(p,q)**: Generalized Autoregressive Conditional Heteroskedasticity\n",
    "\n",
    "$$\\sigma_t^2 = \\omega + \\sum_{i=1}^{p} \\alpha_i \\epsilon_{t-i}^2 + \\sum_{j=1}^{q} \\beta_j \\sigma_{t-j}^2$$\n",
    "\n",
    "**GARCH(1,1)** is most common:\n",
    "$$\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2$$\n",
    "\n",
    "**Properties:**\n",
    "- Models volatility clustering\n",
    "- Captures leverage effects with GJR-GARCH\n",
    "- Essential for option pricing and risk management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0358b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download financial data\n",
    "ticker = 'SPY'\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2024-11-01'\n",
    "\n",
    "print(f\"Downloading {ticker} data...\")\n",
    "data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "prices = data['Close']\n",
    "returns = 100 * prices.pct_change().dropna()  # Returns in percentage\n",
    "\n",
    "print(f\"\\nData Summary:\")\n",
    "print(f\"  Date Range: {returns.index[0]} to {returns.index[-1]}\")\n",
    "print(f\"  Observations: {len(returns)}\")\n",
    "print(f\"\\nReturns Statistics:\")\n",
    "print(returns.describe())\n",
    "\n",
    "# Plot returns\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "ax1.plot(prices.index, prices.values, linewidth=1)\n",
    "ax1.set_title(f'{ticker} Price', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Price ($)', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(returns.index, returns.values, linewidth=0.5, alpha=0.7)\n",
    "ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax2.set_title(f'{ticker} Returns (%)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Returns (%)', fontsize=12)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12187ab",
   "metadata": {},
   "source": [
    "## 2. Fit GARCH(1,1) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552076b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GARCH(1,1) model\n",
    "print(\"\\nFitting GARCH(1,1) model...\")\n",
    "\n",
    "# Using ARCH package best practices\n",
    "model_garch = arch_model(returns, vol='Garch', p=1, q=1, mean='Constant', dist='normal')\n",
    "res_garch = model_garch.fit(disp='off')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GARCH(1,1) ESTIMATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(res_garch.summary())\n",
    "\n",
    "# Extract parameters\n",
    "omega = res_garch.params['omega']\n",
    "alpha = res_garch.params['alpha[1]']\n",
    "beta = res_garch.params['beta[1]']\n",
    "\n",
    "print(f\"\\nKey Parameters:\")\n",
    "print(f\"  Ï‰ (omega): {omega:.6f}\")\n",
    "print(f\"  Î± (alpha): {alpha:.6f}\")\n",
    "print(f\"  Î² (beta):  {beta:.6f}\")\n",
    "print(f\"  Î± + Î²:     {alpha + beta:.6f}  (< 1 for stationarity)\")\n",
    "\n",
    "# Persistence\n",
    "persistence = alpha + beta\n",
    "print(f\"\\nVolatility Persistence: {persistence:.6f}\")\n",
    "print(f\"Half-life of shocks: {np.log(0.5) / np.log(persistence):.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditional volatility\n",
    "cond_vol = res_garch.conditional_volatility\n",
    "\n",
    "# Annualized volatility\n",
    "ann_vol = cond_vol * np.sqrt(252)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Returns and conditional volatility\n",
    "ax = axes[0]\n",
    "ax.plot(returns.index, returns.values, linewidth=0.5, alpha=0.5, label='Returns')\n",
    "ax.plot(cond_vol.index, cond_vol.values, 'r-', linewidth=2, label='Conditional Volatility')\n",
    "ax.plot(cond_vol.index, -cond_vol.values, 'r-', linewidth=2)\n",
    "ax.fill_between(cond_vol.index, -cond_vol.values, cond_vol.values, alpha=0.2, color='red')\n",
    "ax.set_title('Returns with GARCH(1,1) Conditional Volatility', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Returns (%)', fontsize=12)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annualized volatility\n",
    "ax = axes[1]\n",
    "ax.plot(ann_vol.index, ann_vol.values, linewidth=2, color='darkblue')\n",
    "ax.axhline(y=ann_vol.mean(), color='red', linestyle='--', linewidth=2, \n",
    "          label=f'Mean: {ann_vol.mean():.2f}%')\n",
    "ax.set_title('Annualized Volatility (GARCH)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Volatility (%)', fontsize=12)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAnnualized Volatility Statistics:\")\n",
    "print(f\"  Mean: {ann_vol.mean():.2f}%\")\n",
    "print(f\"  Min:  {ann_vol.min():.2f}%\")\n",
    "print(f\"  Max:  {ann_vol.max():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ea332",
   "metadata": {},
   "source": [
    "## 3. GJR-GARCH for Asymmetric Effects\n",
    "\n",
    "**GJR-GARCH** captures leverage effect (negative shocks increase volatility more):\n",
    "\n",
    "$$\\sigma_t^2 = \\omega + (\\alpha + \\gamma I_{t-1})\\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2$$\n",
    "\n",
    "Where $I_{t-1} = 1$ if $\\epsilon_{t-1} < 0$, else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4722ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GJR-GARCH\n",
    "print(\"\\nFitting GJR-GARCH(1,1) model...\")\n",
    "\n",
    "model_gjr = arch_model(returns, vol='Garch', p=1, o=1, q=1, mean='Constant', dist='normal')\n",
    "res_gjr = model_gjr.fit(disp='off')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GJR-GARCH(1,1) ESTIMATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(res_gjr.summary())\n",
    "\n",
    "# Extract gamma (asymmetry parameter)\n",
    "gamma = res_gjr.params['gamma[1]']\n",
    "print(f\"\\nAsymmetry Parameter (Î³): {gamma:.6f}\")\n",
    "print(f\"Interpretation: {'Leverage effect present' if gamma > 0 else 'No leverage effect'}\")\n",
    "\n",
    "# Compare models using AIC/BIC\n",
    "print(f\"\\nModel Comparison:\")\n",
    "print(f\"  GARCH(1,1)     AIC: {res_garch.aic:.2f}  BIC: {res_garch.bic:.2f}\")\n",
    "print(f\"  GJR-GARCH(1,1) AIC: {res_gjr.aic:.2f}  BIC: {res_gjr.bic:.2f}\")\n",
    "print(f\"  Better model: {'GJR-GARCH' if res_gjr.aic < res_garch.aic else 'GARCH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4da3d",
   "metadata": {},
   "source": [
    "## 4. Volatility Forecasting with GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a4609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast volatility\n",
    "forecast_horizon = 30  # days\n",
    "\n",
    "print(f\"\\nForecasting volatility for next {forecast_horizon} days...\")\n",
    "forecasts = res_garch.forecast(horizon=forecast_horizon, reindex=False)\n",
    "\n",
    "# Extract forecast variance\n",
    "forecast_variance = forecasts.variance.values[-1, :]\n",
    "forecast_vol = np.sqrt(forecast_variance)\n",
    "forecast_ann_vol = forecast_vol * np.sqrt(252)\n",
    "\n",
    "# Plot forecast\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Historical volatility\n",
    "hist_vol = ann_vol.iloc[-100:]  # Last 100 days\n",
    "ax.plot(hist_vol.index, hist_vol.values, 'b-', linewidth=2, label='Historical Volatility')\n",
    "\n",
    "# Forecast\n",
    "last_date = hist_vol.index[-1]\n",
    "forecast_dates = pd.date_range(start=last_date, periods=forecast_horizon+1, freq='B')[1:]\n",
    "ax.plot(forecast_dates, forecast_ann_vol, 'r--', linewidth=2, label='GARCH Forecast')\n",
    "\n",
    "# Add confidence interval (approximate)\n",
    "std_error = forecast_ann_vol * 0.1  # Simplified\n",
    "ax.fill_between(forecast_dates, \n",
    "               forecast_ann_vol - 1.96*std_error,\n",
    "               forecast_ann_vol + 1.96*std_error,\n",
    "               alpha=0.2, color='red', label='95% CI')\n",
    "\n",
    "ax.set_title(f'{ticker} Volatility Forecast (GARCH)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Annualized Volatility (%)', fontsize=12)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{forecast_horizon}-Day Forecast:\")\n",
    "print(f\"  Day 1 volatility:  {forecast_ann_vol[0]:.2f}%\")\n",
    "print(f\"  Day {forecast_horizon} volatility: {forecast_ann_vol[-1]:.2f}%\")\n",
    "print(f\"  Mean forecast:     {forecast_ann_vol.mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc25eb",
   "metadata": {},
   "source": [
    "## 5. LSTM for Return Prediction\n",
    "\n",
    "**Long Short-Term Memory (LSTM)** networks can capture complex temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6390eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TensorFlow is available\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    tf_available = True\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    tf_available = False\n",
    "    print(\"TensorFlow not installed. Install with: pip install tensorflow\")\n",
    "\n",
    "if tf_available:\n",
    "    # Prepare data for LSTM\n",
    "    def create_sequences(data, seq_length=60):\n",
    "        \"\"\"\n",
    "        Create sequences for LSTM.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array\n",
    "            Time series data\n",
    "        seq_length : int\n",
    "            Sequence length (lookback period)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X, y : arrays\n",
    "            Features and targets\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            X.append(data[i:i+seq_length])\n",
    "            y.append(data[i+seq_length])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    # Prepare returns data\n",
    "    returns_array = returns.values.reshape(-1, 1)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    returns_scaled = scaler.fit_transform(returns_array)\n",
    "    \n",
    "    # Create sequences\n",
    "    seq_length = 60  # Use 60 days to predict next day\n",
    "    X, y = create_sequences(returns_scaled, seq_length)\n",
    "    \n",
    "    # Train/test split (80/20)\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(f\"\\nLSTM Data Preparation:\")\n",
    "    print(f\"  Sequence length: {seq_length} days\")\n",
    "    print(f\"  Training samples: {len(X_train)}\")\n",
    "    print(f\"  Testing samples: {len(X_test)}\")\n",
    "    print(f\"  Input shape: {X_train.shape}\")\n",
    "    \n",
    "    # Build LSTM model\n",
    "    print(\"\\nBuilding LSTM model...\")\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(seq_length, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining LSTM (this may take a few minutes)...\")\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train, verbose=0)\n",
    "    y_pred_test = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_train_orig = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    y_pred_train_orig = scaler.inverse_transform(y_pred_train)\n",
    "    y_pred_test_orig = scaler.inverse_transform(y_pred_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train_orig, y_pred_train_orig)\n",
    "    test_mse = mean_squared_error(y_test_orig, y_pred_test_orig)\n",
    "    train_mae = mean_absolute_error(y_train_orig, y_pred_train_orig)\n",
    "    test_mae = mean_absolute_error(y_test_orig, y_pred_test_orig)\n",
    "    \n",
    "    print(f\"\\nLSTM Performance:\")\n",
    "    print(f\"  Train MSE: {train_mse:.6f}\")\n",
    "    print(f\"  Test MSE:  {test_mse:.6f}\")\n",
    "    print(f\"  Train MAE: {train_mae:.6f}\")\n",
    "    print(f\"  Test MAE:  {test_mae:.6f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Training history\n",
    "    ax = axes[0]\n",
    "    ax.plot(history.history['loss'], label='Training Loss')\n",
    "    ax.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax.set_title('LSTM Training History', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predictions vs Actual (test set)\n",
    "    ax = axes[1]\n",
    "    test_dates = returns.index[split_idx+seq_length:]\n",
    "    ax.plot(test_dates, y_test_orig, label='Actual Returns', alpha=0.7, linewidth=1)\n",
    "    ax.plot(test_dates, y_pred_test_orig, label='LSTM Predictions', alpha=0.7, linewidth=1)\n",
    "    ax.set_title('LSTM Predictions vs Actual Returns (Test Set)', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Returns (%)', fontsize=12)\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\nSkipping LSTM example. Install TensorFlow to run this section.\")\n",
    "    print(\"Command: pip install tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1792d81",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### GARCH Models\n",
    "1. **Volatility Clustering**: GARCH captures periods of high/low volatility\n",
    "2. **Persistence**: Î± + Î² measures how long shocks persist\n",
    "3. **Leverage Effect**: GJR-GARCH models asymmetric volatility response\n",
    "4. **Forecasting**: GARCH provides conditional volatility forecasts\n",
    "5. **Applications**: Option pricing, VaR, risk management\n",
    "\n",
    "### LSTM Networks\n",
    "1. **Sequence Learning**: Captures long-term dependencies\n",
    "2. **Nonlinear Patterns**: Can model complex relationships\n",
    "3. **Feature Engineering**: Minimal manual feature extraction needed\n",
    "4. **Overfitting**: Requires careful regularization (dropout, early stopping)\n",
    "5. **Limitations**: Not always better than simpler models; hard to interpret\n",
    "\n",
    "### Practical Insights\n",
    "- **GARCH**: Best for volatility forecasting and risk metrics\n",
    "- **LSTM**: Experimental for return prediction; use with caution\n",
    "- **Hybrid**: Combine GARCH volatility with LSTM for features\n",
    "- **Validation**: Always backtest on out-of-sample data\n",
    "- **Production**: GARCH more reliable for production systems\n",
    "\n",
    "**CAREER RELEVANCE:**\n",
    "- GARCH knowledge essential for risk management roles\n",
    "- LSTM shows ML/AI skills valued by quant firms\n",
    "- Understanding both demonstrates versatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb794da8",
   "metadata": {},
   "source": [
    "## ðŸ“ Guided Exercises with Auto-Validation\n",
    "\n",
    "Practice volatility modeling and understand GARCH parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b63677",
   "metadata": {},
   "source": [
    "### Exercise 1: GARCH Persistence and Half-Life (Intermediate)\n",
    "\n",
    "Calculate volatility persistence from GARCH parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377160c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: GARCH Persistence\n",
    "import numpy as np\n",
    "\n",
    "# Given GARCH(1,1) parameters from estimation\n",
    "omega = 0.00001    # Constant term\n",
    "alpha = 0.08       # ARCH coefficient (reaction to shocks)\n",
    "beta = 0.90        # GARCH coefficient (persistence)\n",
    "\n",
    "# TODO: Calculate persistence (Î± + Î²)\n",
    "persistence = None\n",
    "\n",
    "# TODO: Calculate half-life of volatility shocks\n",
    "# Formula: half_life = ln(0.5) / ln(persistence)\n",
    "half_life_days = None\n",
    "\n",
    "# TODO: Check for stationarity (persistence < 1)\n",
    "is_stationary = None  # True or False\n",
    "\n",
    "# ============= AUTO-VALIDATION (DO NOT MODIFY) =============\n",
    "assert persistence is not None, \"âŒ Calculate persistence!\"\n",
    "assert half_life_days is not None, \"âŒ Calculate half-life!\"\n",
    "assert is_stationary is not None, \"âŒ Check stationarity!\"\n",
    "assert persistence == alpha + beta, f\"âŒ Persistence should be Î± + Î² = {alpha + beta}\"\n",
    "expected_half_life = np.log(0.5) / np.log(persistence)\n",
    "assert np.isclose(half_life_days, expected_half_life, rtol=0.01), f\"âŒ Half-life incorrect. Expected {expected_half_life:.2f}, got {half_life_days:.2f}\"\n",
    "assert is_stationary == (persistence < 1), \"âŒ Stationarity check incorrect\"\n",
    "print(\"âœ… Exercise 1 Complete!\")\n",
    "print(f\"   Persistence (Î± + Î²): {persistence:.4f}\")\n",
    "print(f\"   Half-life: {half_life_days:.2f} trading days\")\n",
    "print(f\"   Stationary: {is_stationary}\")\n",
    "print(f\"   Interpretation: Volatility shocks decay by half in ~{half_life_days:.0f} days\")\n",
    "# ========================================================="
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
